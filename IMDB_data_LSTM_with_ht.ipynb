{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba1G1bERoYBW",
        "outputId": "caabde61-41b5-472e-e026-afe2823f82d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.2)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, nlpaug\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nlpaug-1.1.11 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch tensorflow nlpaug scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "import nlpaug.augmenter.word as naw\n"
      ],
      "metadata": {
        "id": "db6ZvpNZpimZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 1. Configuration\n",
        "# ----------------------\n",
        "SEED = 42\n",
        "BATCH_SIZE = 128\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT = 0.5\n",
        "NUM_LAYERS = 2\n",
        "NUM_MODELS = 3\n",
        "MAX_LEN = 200\n",
        "EPOCHS = 100\n",
        "PATIENCE = 5\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "rgSAJsMyqeUS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 2. Enhanced Preprocessing\n",
        "# ----------------------\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "lhq2dIVIqhAg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 3. Data Augmentation\n",
        "# ----------------------\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "def augment_text(text, num_aug=1):\n",
        "    return [aug.augment(text)[0] for _ in range(num_aug)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPSoaIxiqjCF",
        "outputId": "c7a459be-ecc1-4bf6-f1e7-92e98e9ce038"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 4. Dataset Preparation\n",
        "# ----------------------\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, labels, augment=False):\n",
        "        self.reviews = reviews\n",
        "        self.labels = labels\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.reviews[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment and torch.rand(1).item() > 0.5:\n",
        "            text = augment_text(text)[0]\n",
        "\n",
        "        return text, label"
      ],
      "metadata": {
        "id": "SOqpeNHyqkMf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 5. Model Architecture\n",
        "# ----------------------\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attn_weights = torch.softmax(self.attn(lstm_output).squeeze(-1), dim=1)\n",
        "        context = torch.sum(lstm_output * attn_weights.unsqueeze(-1), dim=1)\n",
        "        return context\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix, class_weights):\n",
        "        super().__init__()\n",
        "        self.num_words = embedding_matrix.shape[0]\n",
        "\n",
        "        # Pretrained embeddings\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix), freeze=False\n",
        "        )\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM,\n",
        "                           num_layers=NUM_LAYERS,\n",
        "                           bidirectional=True,\n",
        "                           dropout=DROPOUT,\n",
        "                           batch_first=True)\n",
        "\n",
        "        # Attention\n",
        "        self.attention = Attention(HIDDEN_DIM)\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2, 1)\n",
        "\n",
        "        # Class weights\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        context = self.attention(lstm_out)\n",
        "        context = self.dropout(context)\n",
        "        return torch.sigmoid(self.fc(context)).squeeze()"
      ],
      "metadata": {
        "id": "97OR-zIAqlhw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 6. GloVe Embeddings\n",
        "# ----------------------\n",
        "def load_glove_embeddings(embedding_file):\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index"
      ],
      "metadata": {
        "id": "hYTNaJBWqnce"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "data['review'] = data['review'].apply(clean_text)\n",
        "data['sentiment'] = data['sentiment'].replace({'positive': 1, 'negative': 0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lGb6lyJqoiD",
        "outputId": "2d2258f2-3bdd-4963-bcc4-0d38d550f155"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-a620d98255e8>:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  data['sentiment'] = data['sentiment'].replace({'positive': 1, 'negative': 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=SEED)"
      ],
      "metadata": {
        "id": "SaYxQTlvroqd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_data[\"review\"])"
      ],
      "metadata": {
        "id": "ShaIZ9lErzpz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data[\"review\"]), maxlen=MAX_LEN)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data[\"review\"]), maxlen=MAX_LEN)\n",
        "y_train = train_data[\"sentiment\"].values\n",
        "y_test = test_data[\"sentiment\"].values"
      ],
      "metadata": {
        "id": "5cgUVxBGr6kF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.array([0,1]), y=y_train)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)"
      ],
      "metadata": {
        "id": "bj-hFJoqr9bp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 6. GloVe Embeddings\n",
        "# ----------------------\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "def load_glove_embeddings(embedding_file):\n",
        "    \"\"\"Loads GloVe embeddings from a text file.\n",
        "\n",
        "    Args:\n",
        "        embedding_file (str): Path to the GloVe embeddings file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping words to their corresponding embedding vectors.\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85RXZmNEtNCY",
        "outputId": "f429669b-431c-4f29-e9c8-200ffd983113"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-02 11:55:52--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-03-02 11:55:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-03-02 11:55:52--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2025-03-02 11:58:31 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare GloVe embeddings\n",
        "embeddings_index = load_glove_embeddings(\"glove.6B.300d.txt\")\n",
        "embedding_matrix = np.zeros((10000, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < 10000:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "EDwONRL3ujmq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors\n",
        "X_train_tensor = torch.LongTensor(X_train).to(device)\n",
        "X_test_tensor = torch.LongTensor(X_test).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).to(device)"
      ],
      "metadata": {
        "id": "dx3-_j25sW6I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models\n",
        "models = []\n",
        "for i in range(NUM_MODELS):\n",
        "    model = SentimentLSTM(embedding_matrix, class_weights).to(device)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "id": "86Ff0icyucsN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for each model\n",
        "from torch.utils.data import TensorDataset # import TensorDataset\n",
        "for model_idx, model in enumerate(models):\n",
        "    print(f\"Training model {model_idx+1}/{NUM_MODELS}\")\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
        "    criterion = nn.BCELoss(weight=class_weights[1])\n",
        "\n",
        "    best_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Create DataLoader with augmentation\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Split X_test_tensor and y_test_tensor into smaller batches\n",
        "            VALIDATION_BATCH_SIZE = 32  # Choose a smaller batch size for validation\n",
        "            num_batches = len(X_test_tensor) // VALIDATION_BATCH_SIZE + (len(X_test_tensor) % VALIDATION_BATCH_SIZE != 0)\n",
        "\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for i in range(num_batches):\n",
        "                start_idx = i * VALIDATION_BATCH_SIZE\n",
        "                end_idx = min((i + 1) * VALIDATION_BATCH_SIZE, len(X_test_tensor))\n",
        "\n",
        "                batch_inputs = X_test_tensor[start_idx:end_idx]\n",
        "                batch_labels = y_test_tensor[start_idx:end_idx]\n",
        "\n",
        "                outputs = model(batch_inputs)\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                correct += (predicted == batch_labels).sum().item()\n",
        "                total += batch_labels.size(0)\n",
        "\n",
        "            acc = correct / total\n",
        "\n",
        "        scheduler.step(acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Acc: {acc:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f\"best_model_{model_idx}.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(torch.load(f\"best_model_{model_idx}.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SARh2BcDu19P",
        "outputId": "ec921207-b793-4277-c7cf-a1c9d120f2c7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model 1/3\n",
            "Epoch 1 | Loss: 0.3452 | Acc: 0.8647\n",
            "Epoch 2 | Loss: 0.3047 | Acc: 0.8767\n",
            "Epoch 3 | Loss: 0.2831 | Acc: 0.8833\n",
            "Epoch 4 | Loss: 0.2622 | Acc: 0.8900\n",
            "Epoch 5 | Loss: 0.2518 | Acc: 0.8923\n",
            "Epoch 6 | Loss: 0.2295 | Acc: 0.8871\n",
            "Epoch 7 | Loss: 0.2165 | Acc: 0.8786\n",
            "Epoch 8 | Loss: 0.2073 | Acc: 0.8966\n",
            "Epoch 9 | Loss: 0.1903 | Acc: 0.8978\n",
            "Epoch 10 | Loss: 0.1814 | Acc: 0.8969\n",
            "Epoch 11 | Loss: 0.1685 | Acc: 0.8801\n",
            "Epoch 12 | Loss: 0.1573 | Acc: 0.8931\n",
            "Epoch 13 | Loss: 0.1303 | Acc: 0.9011\n",
            "Epoch 14 | Loss: 0.1264 | Acc: 0.8964\n",
            "Epoch 15 | Loss: 0.1239 | Acc: 0.8992\n",
            "Epoch 16 | Loss: 0.1218 | Acc: 0.8973\n",
            "Epoch 17 | Loss: 0.1176 | Acc: 0.8988\n",
            "Epoch 18 | Loss: 0.1172 | Acc: 0.8983\n",
            "Early stopping triggered\n",
            "Training model 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-e870346928d6>:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{model_idx}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.4898 | Acc: 0.8480\n",
            "Epoch 2 | Loss: 0.3393 | Acc: 0.8550\n",
            "Epoch 3 | Loss: 0.2999 | Acc: 0.8797\n",
            "Epoch 4 | Loss: 0.2732 | Acc: 0.8830\n",
            "Epoch 5 | Loss: 0.2608 | Acc: 0.8825\n",
            "Epoch 6 | Loss: 0.2413 | Acc: 0.8898\n",
            "Epoch 7 | Loss: 0.2240 | Acc: 0.8943\n",
            "Epoch 8 | Loss: 0.2079 | Acc: 0.8915\n",
            "Epoch 9 | Loss: 0.2023 | Acc: 0.8793\n",
            "Epoch 10 | Loss: 0.1861 | Acc: 0.8915\n",
            "Epoch 11 | Loss: 0.1594 | Acc: 0.8969\n",
            "Epoch 12 | Loss: 0.1564 | Acc: 0.8983\n",
            "Epoch 13 | Loss: 0.1537 | Acc: 0.8951\n",
            "Epoch 14 | Loss: 0.1513 | Acc: 0.8979\n",
            "Epoch 15 | Loss: 0.1491 | Acc: 0.8964\n",
            "Epoch 16 | Loss: 0.1451 | Acc: 0.8969\n",
            "Epoch 17 | Loss: 0.1449 | Acc: 0.8965\n",
            "Early stopping triggered\n",
            "Training model 3/3\n",
            "Epoch 1 | Loss: 0.4908 | Acc: 0.8382\n",
            "Epoch 2 | Loss: 0.3334 | Acc: 0.8680\n",
            "Epoch 3 | Loss: 0.2987 | Acc: 0.8783\n",
            "Epoch 4 | Loss: 0.2687 | Acc: 0.8848\n",
            "Epoch 5 | Loss: 0.2612 | Acc: 0.8816\n",
            "Epoch 6 | Loss: 0.2422 | Acc: 0.8880\n",
            "Epoch 7 | Loss: 0.2252 | Acc: 0.8938\n",
            "Epoch 8 | Loss: 0.2144 | Acc: 0.8782\n",
            "Epoch 9 | Loss: 0.2052 | Acc: 0.8700\n",
            "Epoch 10 | Loss: 0.1941 | Acc: 0.8928\n",
            "Epoch 11 | Loss: 0.1635 | Acc: 0.8971\n",
            "Epoch 12 | Loss: 0.1581 | Acc: 0.8970\n",
            "Epoch 13 | Loss: 0.1563 | Acc: 0.8956\n",
            "Epoch 14 | Loss: 0.1541 | Acc: 0.8969\n",
            "Epoch 15 | Loss: 0.1505 | Acc: 0.8988\n",
            "Epoch 16 | Loss: 0.1496 | Acc: 0.8986\n",
            "Epoch 17 | Loss: 0.1493 | Acc: 0.8973\n",
            "Epoch 18 | Loss: 0.1489 | Acc: 0.8987\n",
            "Epoch 19 | Loss: 0.1485 | Acc: 0.8982\n",
            "Epoch 20 | Loss: 0.1487 | Acc: 0.8981\n",
            "Early stopping triggered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 8. Ensemble Prediction\n",
        "# ----------------------\n",
        "def ensemble_predict(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequence, maxlen=MAX_LEN)\n",
        "    tensor = torch.LongTensor(padded).to(device)\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for model in models:\n",
        "            output = model(tensor).item()\n",
        "            predictions.append(output)\n",
        "\n",
        "    avg_pred = np.mean(predictions)\n",
        "    return \"positive\" if avg_pred > 0.5 else \"negative\""
      ],
      "metadata": {
        "id": "7Adsi_0rqr4G"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 9. Evaluation\n",
        "# ----------------------\n",
        "# Evaluate ensemble\n",
        "correct = 0\n",
        "for text, label in zip(test_data[\"review\"], test_data[\"sentiment\"]):\n",
        "    pred = ensemble_predict(text)\n",
        "    correct += (pred == (\"positive\" if label == 1 else \"negative\"))\n",
        "\n",
        "print(f\"Ensemble Accuracy: {correct/len(test_data):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBY8c0OdqxGh",
        "outputId": "96d42f41-276c-4afc-e277-c1a861ddd852"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 0.9046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 10. Example Usage\n",
        "# ----------------------\n",
        "test_reviews = [\n",
        "    \"This movie was an absolute masterpiece!\",\n",
        "    \"Terrible waste of time, would not recommend.\",\n",
        "    \"The plot was average but acting was good.\"\n",
        "]\n",
        "\n",
        "for review in test_reviews:\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Sentiment: {ensemble_predict(review)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDbLQd75qyq4",
        "outputId": "5c7dd033-b5bf-4a5e-8e32-0838ebb1019f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This movie was an absolute masterpiece!\n",
            "Sentiment: positive\n",
            "\n",
            "Review: Terrible waste of time, would not recommend.\n",
            "Sentiment: negative\n",
            "\n",
            "Review: The plot was average but acting was good.\n",
            "Sentiment: negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDodFuB99iz9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}